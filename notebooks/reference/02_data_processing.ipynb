{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the data pipeline from raw tables to analytical datasets. At the end of this activity, train & test data sets are created from raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import os\n",
    "import os.path as op\n",
    "import shutil\n",
    "\n",
    "# standard third party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.options.mode.use_inf_as_na = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard code-template imports\n",
    "from ta_lib.core.api import (\n",
    "    create_context, get_dataframe, get_feature_names_from_column_transformer, get_package_path,\n",
    "    display_as_tabs, string_cleaning, merge_info, initialize_environment,\n",
    "    list_datasets, load_dataset, save_dataset\n",
    ")\n",
    "import ta_lib.eda.api as eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_environment(debug=False, hide_warnings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/raw/housing',\n",
      " '/cleaned/housing',\n",
      " '/train/housing/features',\n",
      " '/train/housing/target',\n",
      " '/test/housing/features',\n",
      " '/test/housing/target',\n",
      " '/score/sales/output']\n"
     ]
    }
   ],
   "source": [
    "config_path = op.join('conf', 'config.yml')\n",
    "context = create_context(config_path)\n",
    "pprint(list_datasets(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = load_dataset(context, \"raw/housing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data cleaning and consolidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u>NOTES</u>**\n",
    "\n",
    "The focus here is to create a cleaned dataset that is appropriate for solving the DS problem at hand from the raw data.\n",
    "\n",
    "**1. Do**\n",
    "* clean dataframe column names\n",
    "* ensure dtypes are set properly\n",
    "* join with other tables etc to create features\n",
    "* transform, if appropriate, datetime like columns to generate additional features (weekday etc)\n",
    "* transform, if appropriate, string columns to generate additional features\n",
    "* discard cols that are not useful for training the model (IDs, constant cols, duplicate cols etc)\n",
    "* additional features generated from existing columns\n",
    "\n",
    "\n",
    "**2. Don't**\n",
    "* handle missing values or outliers here. mark them and leave them for processing downstream.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Clean individual tables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Products Table\n",
    "\n",
    "From data discovery, we know the following\n",
    "\n",
    "* all columns are strings : nothing to fix. Apply generic cleaning (strip extra whitespace etc)\n",
    "* ensure all `invalid` string entries are mapped to np.NaN\n",
    "* some column are duplicates (eg. color, Ext_Color). Better to `coalesce` them instead of an outright discard of one of the columns.\n",
    "* SKU is key column : ensure no duplicate values\n",
    "* This will go into production code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df_clean = (\n",
    "    housing_df\n",
    "    # generating a copy of the original DataFrame as a backup\n",
    "    .copy()\n",
    "    \n",
    "    .replace({'': np.NaN})\n",
    "    \n",
    "    # clean column names\n",
    "    .clean_names(case_type=\"snake\")\n",
    ")\n",
    "housing_df_clean.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "\n",
    "It's always a good idea to save cleaned tabular data using a storage format that supports the following \n",
    "\n",
    "1. preserves the type information\n",
    "2. language agnostic storage format\n",
    "3. Supports compression\n",
    "4. Supports customizing storage to optimize different data access patterns\n",
    "\n",
    "For larger datasets, the last two points become crucial.\n",
    "\n",
    "`Parquet` is one such file format that is very popular for storing tabular data. It has some nice properties:\n",
    "- Similar to pickles & RDS datasets, but compatible with all languages\n",
    "- Preserves the datatypes\n",
    "- Compresses the data and reduces the filesize\n",
    "- Good library support in Python and other languages\n",
    "- As a columnar storage we can efficiently read fewer columns\n",
    "- It also supports chunking data by groups of columns (for instance, by dates or a particular value of a key column) that makes loading subsets of the data fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(context, housing_df_clean, 'cleaned/housing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df_clean.to_parquet(\"../../data/cleaned/housing.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate Train, Validation and Test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We split the data into train, test (optionally, also a validation dataset)\n",
    "- In this example, we are binning the target into 10 quantiles and then use a Stratified Shuffle to split the data.\n",
    "- See sklearn documentation on the various available splitters\n",
    "- https://scikit-learn.org/stable/modules/classes.html#splitter-classes\n",
    "- This will go into production code (training only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from ta_lib.core.api import custom_train_test_split  # helper function to customize splitting\n",
    "from scripts import *\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=context.random_seed)\n",
    "housing_df_train, housing_df_test = custom_train_test_split(housing_df_clean, splitter, by=binned_median_income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"median_house_value\"\n",
    "\n",
    "train_X, train_y = (\n",
    "    housing_df_train\n",
    "    .get_features_targets(target_column_names=target_col)\n",
    ")\n",
    "\n",
    "save_dataset(context, train_X, \"train/housing/features\")\n",
    "save_dataset(context, train_y, \"train/housing/target\")\n",
    "\n",
    "test_X, test_y = (\n",
    "    housing_df_test\n",
    "    .get_features_targets(target_column_names=target_col)\n",
    ")\n",
    "\n",
    "save_dataset(context, test_X, 'test/housing/features')\n",
    "save_dataset(context, test_y, 'test/housing/target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
